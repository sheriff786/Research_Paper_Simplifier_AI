{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# üéØ Stage 1: PDF Ingestion & Structure Detection\n",
    "## Research Paper Simplifier - Backend Development\n",
    "\n",
    "**Goal:** Upload PDF ‚Üí Extract text ‚Üí Detect sections ‚Üí Understand structure with AI\n",
    "\n",
    "**What we'll build:**\n",
    "1. PDF text extraction\n",
    "2. Rule-based section detection\n",
    "3. AI-powered structure analysis with CrewAI\n",
    "4. Metadata extraction\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üì¶ Step 1: Imports and Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚ùå OPENAI_API_KEY not found!\n",
      "Please create a .env file with: OPENAI_API_KEY=your_key_here\n"
     ]
    }
   ],
   "source": [
    "# Imports\n",
    "import os\n",
    "import json\n",
    "import re\n",
    "from pathlib import Path\n",
    "from typing import Dict, List, Optional\n",
    "from datetime import datetime\n",
    "\n",
    "# PDF processing\n",
    "# import fitz  # PyMuPDF\n",
    "import pdfplumber\n",
    "\n",
    "# AI and LLM\n",
    "from crewai import Agent, Task, Crew, Process\n",
    "from langchain_openai import ChatOpenAI\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "# Load environment variables\n",
    "load_dotenv()\n",
    "\n",
    "# Verify API key\n",
    "if not os.getenv(\"OPENAI_API_KEY\"):\n",
    "    print(\"‚ùå OPENAI_API_KEY not found!\")\n",
    "    print(\"Please create a .env file with: OPENAI_API_KEY=your_key_here\")\n",
    "else:\n",
    "    print(\"‚úÖ Environment setup complete!\")\n",
    "    print(f\"‚úÖ OpenAI API Key loaded: {os.getenv('OPENAI_API_KEY')[:10]}...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üìÑ Step 2: PDF Text Extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PDFExtractor:\n",
    "    \"\"\"Extract text and metadata from PDF files\"\"\"\n",
    "    \n",
    "    def __init__(self, pdf_path: str):\n",
    "        self.pdf_path = pdf_path\n",
    "        self.metadata = {}\n",
    "        self.full_text = \"\"\n",
    "        self.pages = []\n",
    "        \n",
    "    def extract_with_pymupdf(self) -> Dict:\n",
    "        \"\"\"Extract text using PyMuPDF (fast and reliable)\"\"\"\n",
    "        print(f\"üìÑ Extracting text from: {self.pdf_path}\")\n",
    "        \n",
    "        doc = fitz.open(self.pdf_path)\n",
    "        \n",
    "        # Extract metadata\n",
    "        self.metadata = {\n",
    "            \"title\": doc.metadata.get(\"title\", \"Unknown\"),\n",
    "            \"author\": doc.metadata.get(\"author\", \"Unknown\"),\n",
    "            \"pages\": doc.page_count,\n",
    "            \"created\": doc.metadata.get(\"creationDate\", \"Unknown\"),\n",
    "        }\n",
    "        \n",
    "        # Extract text from each page\n",
    "        for page_num, page in enumerate(doc, start=1):\n",
    "            page_text = page.get_text()\n",
    "            self.pages.append({\n",
    "                \"page_number\": page_num,\n",
    "                \"text\": page_text,\n",
    "                \"char_count\": len(page_text)\n",
    "            })\n",
    "            self.full_text += page_text + \"\\n\"\n",
    "        \n",
    "        doc.close()\n",
    "        \n",
    "        print(f\"‚úÖ Extracted {len(self.pages)} pages\")\n",
    "        print(f\"‚úÖ Total characters: {len(self.full_text):,}\")\n",
    "        \n",
    "        return {\n",
    "            \"metadata\": self.metadata,\n",
    "            \"full_text\": self.full_text,\n",
    "            \"pages\": self.pages\n",
    "        }\n",
    "\n",
    "print(\"‚úÖ PDFExtractor class defined\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üîç Step 3: Section Detection (Rule-based)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SectionDetector:\n",
    "    \"\"\"Detect common research paper sections using pattern matching\"\"\"\n",
    "    \n",
    "    # Common section headers in research papers\n",
    "    SECTION_PATTERNS = {\n",
    "        \"abstract\": [\n",
    "            r\"^abstract\\s*$\",\n",
    "            r\"^summary\\s*$\",\n",
    "        ],\n",
    "        \"introduction\": [\n",
    "            r\"^1\\.?\\s*introduction\",\n",
    "            r\"^introduction\\s*$\",\n",
    "            r\"^i\\.?\\s*introduction\",\n",
    "        ],\n",
    "        \"related_work\": [\n",
    "            r\"^2\\.?\\s*related work\",\n",
    "            r\"^related work\\s*$\",\n",
    "            r\"^literature review\",\n",
    "            r\"^background\",\n",
    "        ],\n",
    "        \"methodology\": [\n",
    "            r\"^3\\.?\\s*method\",\n",
    "            r\"^methodology\\s*$\",\n",
    "            r\"^approach\\s*$\",\n",
    "            r\"^model\\s*$\",\n",
    "        ],\n",
    "        \"results\": [\n",
    "            r\"^4\\.?\\s*results\",\n",
    "            r\"^results\\s*$\",\n",
    "            r\"^experiments\\s*$\",\n",
    "            r\"^evaluation\\s*$\",\n",
    "        ],\n",
    "        \"discussion\": [\n",
    "            r\"^5\\.?\\s*discussion\",\n",
    "            r\"^discussion\\s*$\",\n",
    "            r\"^analysis\\s*$\",\n",
    "        ],\n",
    "        \"conclusion\": [\n",
    "            r\"^6\\.?\\s*conclusion\",\n",
    "            r\"^conclusion\\s*$\",\n",
    "            r\"^concluding remarks\",\n",
    "        ],\n",
    "        \"references\": [\n",
    "            r\"^references\\s*$\",\n",
    "            r\"^bibliography\\s*$\",\n",
    "        ],\n",
    "    }\n",
    "    \n",
    "    def __init__(self, text: str):\n",
    "        self.text = text\n",
    "        self.sections = {}\n",
    "        \n",
    "    def detect_sections(self) -> Dict[str, str]:\n",
    "        \"\"\"Detect sections using pattern matching\"\"\"\n",
    "        lines = self.text.split('\\n')\n",
    "        current_section = \"unknown\"\n",
    "        section_content = {key: [] for key in self.SECTION_PATTERNS.keys()}\n",
    "        section_content[\"unknown\"] = []\n",
    "        \n",
    "        for line in lines:\n",
    "            line_lower = line.strip().lower()\n",
    "            \n",
    "            # Check if line matches any section header\n",
    "            matched = False\n",
    "            for section_name, patterns in self.SECTION_PATTERNS.items():\n",
    "                for pattern in patterns:\n",
    "                    if re.match(pattern, line_lower, re.IGNORECASE):\n",
    "                        current_section = section_name\n",
    "                        matched = True\n",
    "                        break\n",
    "                if matched:\n",
    "                    break\n",
    "            \n",
    "            # Add line to current section\n",
    "            if not matched and line.strip():\n",
    "                section_content[current_section].append(line)\n",
    "        \n",
    "        # Convert lists to strings\n",
    "        self.sections = {\n",
    "            section: '\\n'.join(content).strip()\n",
    "            for section, content in section_content.items()\n",
    "            if content and '\\n'.join(content).strip()\n",
    "        }\n",
    "        \n",
    "        print(f\"‚úÖ Detected sections: {list(self.sections.keys())}\")\n",
    "        return self.sections\n",
    "\n",
    "print(\"‚úÖ SectionDetector class defined\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ü§ñ Step 4: CrewAI Agents Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize LLM\n",
    "llm = ChatOpenAI(\n",
    "    model=\"gpt-4o-mini\",  # Faster and cheaper for testing\n",
    "    temperature=0.1,  # Low temperature for consistent results\n",
    ")\n",
    "\n",
    "# Agent 1: Paper Structure Analyzer\n",
    "paper_analyzer = Agent(\n",
    "    role=\"Research Paper Structure Analyst\",\n",
    "    goal=\"Analyze the structure of academic papers and identify key sections accurately\",\n",
    "    backstory=\"\"\"You are an expert at reading academic papers across all disciplines.\n",
    "    You have a PhD-level understanding of paper structure and can identify sections\n",
    "    even when they're not clearly labeled. You understand that papers might use\n",
    "    different terminology (e.g., 'Methods' vs 'Methodology' vs 'Approach').\"\"\",\n",
    "    llm=llm,\n",
    "    verbose=True,\n",
    "    allow_delegation=False,\n",
    ")\n",
    "\n",
    "# Agent 2: Content Extractor\n",
    "content_extractor = Agent(\n",
    "    role=\"Academic Content Extractor\",\n",
    "    goal=\"Extract and organize key information from research papers\",\n",
    "    backstory=\"\"\"You are skilled at extracting metadata, key points, and essential\n",
    "    information from academic papers. You can identify the research problem, \n",
    "    contributions, methodology, and results even when they're not explicitly stated.\"\"\",\n",
    "    llm=llm,\n",
    "    verbose=True,\n",
    "    allow_delegation=False,\n",
    ")\n",
    "\n",
    "print(\"‚úÖ CrewAI Agents created:\")\n",
    "print(f\"  - {paper_analyzer.role}\")\n",
    "print(f\"  - {content_extractor.role}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üìã Step 5: Define Tasks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_structure_analysis_task(paper_text: str) -> Task:\n",
    "    \"\"\"Task for analyzing paper structure\"\"\"\n",
    "    return Task(\n",
    "        description=f\"\"\"Analyze this research paper and identify its structure.\n",
    "        \n",
    "        Paper text (first 3000 characters):\n",
    "        ```\n",
    "        {paper_text[:3000]}\n",
    "        ```\n",
    "        \n",
    "        Your task:\n",
    "        1. Identify all major sections (Abstract, Introduction, Methods, Results, etc.)\n",
    "        2. Note any unconventional section names\n",
    "        3. Estimate where each section begins and ends\n",
    "        4. Identify if this is a standard research paper or a different format\n",
    "        \n",
    "        Return a JSON object with this structure:\n",
    "        {{\n",
    "            \"paper_type\": \"research_article|review|conference_paper|preprint\",\n",
    "            \"sections_found\": [\"section1\", \"section2\", ...],\n",
    "            \"structure_quality\": \"high|medium|low\",\n",
    "            \"notes\": \"any important observations\"\n",
    "        }}\n",
    "        \"\"\",\n",
    "        agent=paper_analyzer,\n",
    "        expected_output=\"A JSON object describing the paper structure\"\n",
    "    )\n",
    "\n",
    "def create_metadata_extraction_task(paper_text: str) -> Task:\n",
    "    \"\"\"Task for extracting paper metadata\"\"\"\n",
    "    return Task(\n",
    "        description=f\"\"\"Extract key metadata from this research paper.\n",
    "        \n",
    "        Paper text (first 2000 characters):\n",
    "        ```\n",
    "        {paper_text[:2000]}\n",
    "        ```\n",
    "        \n",
    "        Your task:\n",
    "        1. Extract the paper title\n",
    "        2. Identify authors (if mentioned)\n",
    "        3. Determine the research domain/field\n",
    "        4. Identify the main research problem\n",
    "        5. Note any key contributions mentioned in abstract\n",
    "        \n",
    "        Return a JSON object with this structure:\n",
    "        {{\n",
    "            \"title\": \"paper title\",\n",
    "            \"authors\": [\"author1\", \"author2\"],\n",
    "            \"field\": \"research field\",\n",
    "            \"problem\": \"main research problem\",\n",
    "            \"contributions\": [\"contribution1\", \"contribution2\"]\n",
    "        }}\n",
    "        \"\"\",\n",
    "        agent=content_extractor,\n",
    "        expected_output=\"A JSON object with paper metadata\"\n",
    "    )\n",
    "\n",
    "print(\"‚úÖ Task creation functions defined\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üîÑ Step 6: Complete Processing Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PaperProcessor:\n",
    "    \"\"\"Complete pipeline for processing research papers\"\"\"\n",
    "    \n",
    "    def __init__(self, pdf_path: str):\n",
    "        self.pdf_path = pdf_path\n",
    "        self.extractor = PDFExtractor(pdf_path)\n",
    "        self.extracted_data = None\n",
    "        self.detected_sections = None\n",
    "        self.ai_analysis = None\n",
    "        \n",
    "    def extract_text(self):\n",
    "        \"\"\"Step 1: Extract text from PDF\"\"\"\n",
    "        print(\"\\n\" + \"=\"*50)\n",
    "        print(\"STEP 1: Text Extraction\")\n",
    "        print(\"=\"*50)\n",
    "        self.extracted_data = self.extractor.extract_with_pymupdf()\n",
    "        return self.extracted_data\n",
    "    \n",
    "    def detect_sections(self):\n",
    "        \"\"\"Step 2: Detect sections using rules\"\"\"\n",
    "        print(\"\\n\" + \"=\"*50)\n",
    "        print(\"STEP 2: Section Detection (Rule-based)\")\n",
    "        print(\"=\"*50)\n",
    "        detector = SectionDetector(self.extracted_data[\"full_text\"])\n",
    "        self.detected_sections = detector.detect_sections()\n",
    "        \n",
    "        print(\"\\nSection Summary:\")\n",
    "        for section, content in self.detected_sections.items():\n",
    "            word_count = len(content.split())\n",
    "            print(f\"  üìÑ {section.upper()}: {word_count} words\")\n",
    "        \n",
    "        return self.detected_sections\n",
    "    \n",
    "    def analyze_with_ai(self):\n",
    "        \"\"\"Step 3: Analyze with CrewAI agents\"\"\"\n",
    "        print(\"\\n\" + \"=\"*50)\n",
    "        print(\"STEP 3: AI Analysis with CrewAI\")\n",
    "        print(\"=\"*50)\n",
    "        \n",
    "        # Create tasks\n",
    "        structure_task = create_structure_analysis_task(\n",
    "            self.extracted_data[\"full_text\"]\n",
    "        )\n",
    "        metadata_task = create_metadata_extraction_task(\n",
    "            self.extracted_data[\"full_text\"]\n",
    "        )\n",
    "        \n",
    "        # Create crew\n",
    "        crew = Crew(\n",
    "            agents=[paper_analyzer, content_extractor],\n",
    "            tasks=[structure_task, metadata_task],\n",
    "            process=Process.sequential,\n",
    "            verbose=True\n",
    "        )\n",
    "        \n",
    "        # Execute\n",
    "        print(\"\\nü§ñ Running AI agents...\")\n",
    "        result = crew.kickoff()\n",
    "        \n",
    "        self.ai_analysis = result\n",
    "        return result\n",
    "    \n",
    "    def get_final_output(self) -> Dict:\n",
    "        \"\"\"Get complete processed output\"\"\"\n",
    "        return {\n",
    "            \"pdf_path\": self.pdf_path,\n",
    "            \"processed_at\": datetime.now().isoformat(),\n",
    "            \"metadata\": self.extracted_data[\"metadata\"],\n",
    "            \"page_count\": len(self.extracted_data[\"pages\"]),\n",
    "            \"total_chars\": len(self.extracted_data[\"full_text\"]),\n",
    "            \"detected_sections\": {\n",
    "                section: {\n",
    "                    \"word_count\": len(content.split()),\n",
    "                    \"preview\": content[:200] + \"...\"\n",
    "                }\n",
    "                for section, content in self.detected_sections.items()\n",
    "            },\n",
    "            \"ai_analysis\": str(self.ai_analysis),\n",
    "            \"sections_full\": self.detected_sections\n",
    "        }\n",
    "    \n",
    "    def process(self):\n",
    "        \"\"\"Run complete pipeline\"\"\"\n",
    "        self.extract_text()\n",
    "        self.detect_sections()\n",
    "        self.analyze_with_ai()\n",
    "        return self.get_final_output()\n",
    "\n",
    "print(\"‚úÖ PaperProcessor class defined\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üß™ Step 7: Test with Your PDF\n",
    "\n",
    "**IMPORTANT:** Update the `PDF_PATH` variable below with the path to your research paper PDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# UPDATE THIS PATH TO YOUR PDF!\n",
    "PDF_PATH = \"sample_paper.pdf\"  # Change this to your actual PDF path\n",
    "\n",
    "# Check if file exists\n",
    "if not os.path.exists(PDF_PATH):\n",
    "    print(f\"‚ùå PDF not found: {PDF_PATH}\")\n",
    "    print(\"\"\"\\nPlease:\n",
    "    1. Download a research paper PDF\n",
    "    2. Place it in the same folder as this notebook\n",
    "    3. Update PDF_PATH variable above\n",
    "    \"\"\")\n",
    "else:\n",
    "    print(f\"‚úÖ Found PDF: {PDF_PATH}\")\n",
    "    \n",
    "    # Process the paper\n",
    "    processor = PaperProcessor(PDF_PATH)\n",
    "    \n",
    "    # Run pipeline\n",
    "    result = processor.process()\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*50)\n",
    "    print(\"FINAL RESULT\")\n",
    "    print(\"=\"*50)\n",
    "    print(f\"\\nüìÑ Paper: {result['metadata']['title']}\")\n",
    "    print(f\"üìä Pages: {result['page_count']}\")\n",
    "    print(f\"üìù Total characters: {result['total_chars']:,}\")\n",
    "    print(f\"\\nüîç Detected sections: {list(result['detected_sections'].keys())}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üíæ Step 8: Save Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_results(result: Dict, output_path: str):\n",
    "    \"\"\"Save processed results to JSON file\"\"\"\n",
    "    with open(output_path, 'w', encoding='utf-8') as f:\n",
    "        json.dump(result, f, indent=2, ensure_ascii=False)\n",
    "    print(f\"‚úÖ Results saved to: {output_path}\")\n",
    "\n",
    "# Save if we have results\n",
    "if 'result' in locals():\n",
    "    output_file = \"stage1_output.json\"\n",
    "    save_results(result, output_file)\n",
    "    \n",
    "    # Show sample output\n",
    "    print(\"\\nüìã Sample Output:\")\n",
    "    print(json.dumps(result[\"detected_sections\"], indent=2)[:500])\n",
    "    print(\"\\n[Output truncated... see full results in stage1_output.json]\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üìä Step 9: Visualize Section Distribution (Optional)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def visualize_sections(sections: Dict):\n",
    "    \"\"\"Create a simple bar chart of section sizes\"\"\"\n",
    "    section_names = list(sections.keys())\n",
    "    word_counts = [len(sections[s].split()) for s in section_names]\n",
    "    \n",
    "    plt.figure(figsize=(10, 6))\n",
    "    plt.barh(section_names, word_counts, color='skyblue')\n",
    "    plt.xlabel('Word Count')\n",
    "    plt.ylabel('Section')\n",
    "    plt.title('Research Paper Section Distribution')\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "# Visualize if we have results\n",
    "if 'result' in locals() and result.get('sections_full'):\n",
    "    visualize_sections(result['sections_full'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üéØ Summary\n",
    "\n",
    "**What we accomplished:**\n",
    "- ‚úÖ Extracted text from PDF using PyMuPDF\n",
    "- ‚úÖ Detected sections using rule-based patterns\n",
    "- ‚úÖ Analyzed structure with CrewAI agents\n",
    "- ‚úÖ Extracted metadata intelligently\n",
    "- ‚úÖ Saved results to JSON\n",
    "\n",
    "**Next steps (Stage 2):**\n",
    "1. Implement smart text chunking\n",
    "2. Generate embeddings\n",
    "3. Store in vector database\n",
    "4. Enable semantic search\n",
    "\n",
    "---\n",
    "\n",
    "**Ready to move to Stage 2?** Let me know!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".res",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
